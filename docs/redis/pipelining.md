# Redis ç®¡é“ï¼ˆPipeliningï¼‰

Redis ç®¡é“æ˜¯ä¸€ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œå…è®¸å®¢æˆ·ç«¯ä¸€æ¬¡æ€§å‘é€å¤šä¸ªå‘½ä»¤åˆ°æœåŠ¡å™¨ï¼Œç„¶åä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰å“åº”ï¼Œå¤§å¤§å‡å°‘ç½‘ç»œå¾€è¿”æ—¶é—´ï¼Œæé«˜æ€§èƒ½ã€‚

## ğŸ¯ ç®¡é“æ¦‚è¿°

### ä»€ä¹ˆæ˜¯ Redis ç®¡é“ï¼Ÿ
ç®¡é“æ˜¯ä¸€ç§ç½‘ç»œä¼˜åŒ–æŠ€æœ¯ï¼Œå…è®¸å®¢æˆ·ç«¯åœ¨ä¸ç­‰å¾…æœåŠ¡å™¨å“åº”çš„æƒ…å†µä¸‹å‘é€å¤šä¸ªå‘½ä»¤ï¼Œç„¶åæ‰¹é‡æ¥æ”¶æ‰€æœ‰å“åº”ã€‚

### å·¥ä½œåŸç†
```bash
# æ™®é€šæ¨¡å¼ï¼ˆæ¯ä¸ªå‘½ä»¤éƒ½éœ€è¦ç­‰å¾…å“åº”ï¼‰
å®¢æˆ·ç«¯: SET key1 value1 -> æœåŠ¡å™¨
å®¢æˆ·ç«¯: <- OK              æœåŠ¡å™¨
å®¢æˆ·ç«¯: SET key2 value2 -> æœåŠ¡å™¨  
å®¢æˆ·ç«¯: <- OK              æœåŠ¡å™¨
å®¢æˆ·ç«¯: GET key1        -> æœåŠ¡å™¨
å®¢æˆ·ç«¯: <- value1          æœåŠ¡å™¨

# ç®¡é“æ¨¡å¼ï¼ˆæ‰¹é‡å‘é€ï¼Œæ‰¹é‡æ¥æ”¶ï¼‰
å®¢æˆ·ç«¯: SET key1 value1 -> æœåŠ¡å™¨
å®¢æˆ·ç«¯: SET key2 value2 -> æœåŠ¡å™¨
å®¢æˆ·ç«¯: GET key1        -> æœåŠ¡å™¨
å®¢æˆ·ç«¯: <- OK              æœåŠ¡å™¨
å®¢æˆ·ç«¯: <- OK              æœåŠ¡å™¨
å®¢æˆ·ç«¯: <- value1          æœåŠ¡å™¨
```

### æ ¸å¿ƒä¼˜åŠ¿
- **å‡å°‘ç½‘ç»œå»¶è¿Ÿ**ï¼šå‡å°‘å®¢æˆ·ç«¯ä¸æœåŠ¡å™¨é—´çš„å¾€è¿”æ¬¡æ•°
- **æé«˜ååé‡**ï¼šæ‰¹é‡å¤„ç†å‘½ä»¤ï¼Œæé«˜æ•´ä½“æ€§èƒ½
- **ä¿æŒåŸå­æ€§**ï¼šæ¯ä¸ªå‘½ä»¤ä»ç„¶æ˜¯åŸå­æ‰§è¡Œçš„
- **ç®€å•æ˜“ç”¨**ï¼šå¤§å¤šæ•° Redis å®¢æˆ·ç«¯éƒ½æ”¯æŒç®¡é“

### é€‚ç”¨åœºæ™¯
- æ‰¹é‡æ•°æ®æ“ä½œ
- åˆå§‹åŒ–æ•°æ®
- æ‰¹é‡æŸ¥è¯¢
- æ€§èƒ½æ•æ„Ÿçš„åº”ç”¨

## ğŸ“ åŸºæœ¬ä½¿ç”¨

### å‘½ä»¤è¡Œç®¡é“
```bash
# ä½¿ç”¨ redis-cli çš„ç®¡é“åŠŸèƒ½
echo -e "SET key1 value1\nSET key2 value2\nGET key1\nGET key2" | redis-cli --pipe

# ä»æ–‡ä»¶è¯»å–å‘½ä»¤
cat commands.txt | redis-cli --pipe

# commands.txt å†…å®¹ç¤ºä¾‹ï¼š
SET user:1001 "å¼ ä¸‰"
SET user:1002 "æå››"
HSET user:1001:profile name "å¼ ä¸‰" age 25
HSET user:1002:profile name "æå››" age 30
INCR stats:total_users
INCR stats:total_users
```

### æ€§èƒ½å¯¹æ¯”
```bash
# æµ‹è¯•æ™®é€šæ¨¡å¼æ€§èƒ½
time for i in {1..1000}; do redis-cli SET key$i value$i; done

# æµ‹è¯•ç®¡é“æ¨¡å¼æ€§èƒ½
time for i in {1..1000}; do echo "SET key$i value$i"; done | redis-cli --pipe

# ç®¡é“æ¨¡å¼é€šå¸¸æ¯”æ™®é€šæ¨¡å¼å¿« 5-10 å€
```

## ğŸ”§ å®¢æˆ·ç«¯å®ç°

### Python å®ç°
```python
import redis
import time

# è¿æ¥ Redis
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

# æ™®é€šæ¨¡å¼
def normal_operations():
    start_time = time.time()
    
    for i in range(1000):
        r.set(f'key:{i}', f'value:{i}')
        r.get(f'key:{i}')
    
    end_time = time.time()
    print(f"æ™®é€šæ¨¡å¼è€—æ—¶: {end_time - start_time:.2f} ç§’")

# ç®¡é“æ¨¡å¼
def pipeline_operations():
    start_time = time.time()
    
    pipe = r.pipeline()
    for i in range(1000):
        pipe.set(f'key:{i}', f'value:{i}')
        pipe.get(f'key:{i}')
    
    results = pipe.execute()
    
    end_time = time.time()
    print(f"ç®¡é“æ¨¡å¼è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"è¿”å›ç»“æœæ•°é‡: {len(results)}")

# æ‰¹é‡æ“ä½œç¤ºä¾‹
def batch_user_creation():
    users = [
        {'id': 1001, 'name': 'å¼ ä¸‰', 'email': 'zhangsan@example.com'},
        {'id': 1002, 'name': 'æå››', 'email': 'lisi@example.com'},
        {'id': 1003, 'name': 'ç‹äº”', 'email': 'wangwu@example.com'},
    ]
    
    pipe = r.pipeline()
    
    for user in users:
        # è®¾ç½®ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        pipe.hset(f"user:{user['id']}", mapping={
            'name': user['name'],
            'email': user['email']
        })
        
        # æ·»åŠ åˆ°ç”¨æˆ·é›†åˆ
        pipe.sadd('users:all', user['id'])
        
        # å¢åŠ ç”¨æˆ·è®¡æ•°
        pipe.incr('stats:total_users')
    
    results = pipe.execute()
    print(f"æ‰¹é‡åˆ›å»ºç”¨æˆ·å®Œæˆï¼Œæ‰§è¡Œäº† {len(results)} ä¸ªå‘½ä»¤")
```

### Node.js å®ç°
```javascript
const redis = require('redis');
const client = redis.createClient();

// æ™®é€šæ¨¡å¼
async function normalOperations() {
    const start = Date.now();
    
    for (let i = 0; i < 1000; i++) {
        await client.set(`key:${i}`, `value:${i}`);
        await client.get(`key:${i}`);
    }
    
    const end = Date.now();
    console.log(`æ™®é€šæ¨¡å¼è€—æ—¶: ${end - start} æ¯«ç§’`);
}

// ç®¡é“æ¨¡å¼
async function pipelineOperations() {
    const start = Date.now();
    
    const pipeline = client.multi();
    
    for (let i = 0; i < 1000; i++) {
        pipeline.set(`key:${i}`, `value:${i}`);
        pipeline.get(`key:${i}`);
    }
    
    const results = await pipeline.exec();
    
    const end = Date.now();
    console.log(`ç®¡é“æ¨¡å¼è€—æ—¶: ${end - start} æ¯«ç§’`);
    console.log(`è¿”å›ç»“æœæ•°é‡: ${results.length}`);
}

// æ‰¹é‡æ•°æ®å¤„ç†
async function batchDataProcessing() {
    const pipeline = client.multi();
    
    // æ‰¹é‡è®¾ç½®ç”¨æˆ·æ•°æ®
    const users = [
        {id: 1001, name: 'å¼ ä¸‰', score: 100},
        {id: 1002, name: 'æå››', score: 200},
        {id: 1003, name: 'ç‹äº”', score: 150}
    ];
    
    users.forEach(user => {
        pipeline.hSet(`user:${user.id}`, {
            name: user.name,
            score: user.score
        });
        pipeline.zAdd('leaderboard', {
            score: user.score,
            value: user.id
        });
    });
    
    const results = await pipeline.exec();
    console.log('æ‰¹é‡æ•°æ®å¤„ç†å®Œæˆ');
}
```

### Java å®ç°
```java
import redis.clients.jedis.Jedis;
import redis.clients.jedis.Pipeline;

public class RedisPipelineExample {
    
    public static void main(String[] args) {
        Jedis jedis = new Jedis("localhost", 6379);
        
        // æ™®é€šæ¨¡å¼
        normalOperations(jedis);
        
        // ç®¡é“æ¨¡å¼
        pipelineOperations(jedis);
        
        jedis.close();
    }
    
    // æ™®é€šæ¨¡å¼
    public static void normalOperations(Jedis jedis) {
        long start = System.currentTimeMillis();
        
        for (int i = 0; i < 1000; i++) {
            jedis.set("key:" + i, "value:" + i);
            jedis.get("key:" + i);
        }
        
        long end = System.currentTimeMillis();
        System.out.println("æ™®é€šæ¨¡å¼è€—æ—¶: " + (end - start) + " æ¯«ç§’");
    }
    
    // ç®¡é“æ¨¡å¼
    public static void pipelineOperations(Jedis jedis) {
        long start = System.currentTimeMillis();
        
        Pipeline pipeline = jedis.pipelined();
        
        for (int i = 0; i < 1000; i++) {
            pipeline.set("key:" + i, "value:" + i);
            pipeline.get("key:" + i);
        }
        
        List<Object> results = pipeline.syncAndReturnAll();
        
        long end = System.currentTimeMillis();
        System.out.println("ç®¡é“æ¨¡å¼è€—æ—¶: " + (end - start) + " æ¯«ç§’");
        System.out.println("è¿”å›ç»“æœæ•°é‡: " + results.size());
    }
}
```

## ğŸ¯ åº”ç”¨åœºæ™¯

### 1. æ‰¹é‡æ•°æ®å¯¼å…¥
```python
def import_products(products):
    """æ‰¹é‡å¯¼å…¥å•†å“æ•°æ®"""
    pipe = r.pipeline()
    
    for product in products:
        # è®¾ç½®å•†å“åŸºæœ¬ä¿¡æ¯
        pipe.hset(f"product:{product['id']}", mapping={
            'name': product['name'],
            'price': product['price'],
            'category': product['category'],
            'stock': product['stock']
        })
        
        # æ·»åŠ åˆ°åˆ†ç±»é›†åˆ
        pipe.sadd(f"category:{product['category']}", product['id'])
        
        # æ·»åŠ åˆ°ä»·æ ¼æ’åºé›†åˆ
        pipe.zadd('products:by_price', {product['id']: product['price']})
        
        # æ›´æ–°ç»Ÿè®¡
        pipe.incr('stats:total_products')
        pipe.incrby('stats:total_value', product['price'] * product['stock'])
    
    results = pipe.execute()
    return len(results)
```

### 2. æ‰¹é‡ç¼“å­˜é¢„çƒ­
```python
def cache_warmup(user_ids):
    """æ‰¹é‡é¢„çƒ­ç”¨æˆ·ç¼“å­˜"""
    pipe = r.pipeline()
    
    for user_id in user_ids:
        # ä»æ•°æ®åº“è·å–ç”¨æˆ·æ•°æ®ï¼ˆè¿™é‡Œæ¨¡æ‹Ÿï¼‰
        user_data = get_user_from_db(user_id)
        
        if user_data:
            # ç¼“å­˜ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
            pipe.setex(f"cache:user:{user_id}", 3600, 
                      json.dumps(user_data))
            
            # ç¼“å­˜ç”¨æˆ·æƒé™
            pipe.setex(f"cache:permissions:{user_id}", 1800,
                      json.dumps(user_data['permissions']))
            
            # è®°å½•ç¼“å­˜æ—¶é—´
            pipe.hset(f"cache:meta:{user_id}", 
                     'cached_at', int(time.time()))
    
    pipe.execute()
```

### 3. æ‰¹é‡ç»Ÿè®¡æ›´æ–°
```python
def update_daily_stats(events):
    """æ‰¹é‡æ›´æ–°æ¯æ—¥ç»Ÿè®¡"""
    pipe = r.pipeline()
    today = datetime.now().strftime('%Y-%m-%d')
    
    for event in events:
        # æ›´æ–°ç”¨æˆ·æ´»è·ƒåº¦
        pipe.sadd(f"active_users:{today}", event['user_id'])
        
        # æ›´æ–°äº‹ä»¶è®¡æ•°
        pipe.incr(f"events:{event['type']}:{today}")
        
        # æ›´æ–°å°æ—¶ç»Ÿè®¡
        hour = datetime.now().strftime('%Y-%m-%d:%H')
        pipe.incr(f"events:hourly:{hour}")
        
        # æ›´æ–°ç”¨æˆ·è¡Œä¸º
        pipe.lpush(f"user_events:{event['user_id']}", 
                  json.dumps(event))
        pipe.ltrim(f"user_events:{event['user_id']}", 0, 99)  # ä¿ç•™æœ€è¿‘100æ¡
    
    pipe.execute()
```

### 4. æ‰¹é‡æ¸…ç†è¿‡æœŸæ•°æ®
```python
def cleanup_expired_data():
    """æ‰¹é‡æ¸…ç†è¿‡æœŸæ•°æ®"""
    pipe = r.pipeline()
    
    # è·å–éœ€è¦æ¸…ç†çš„é”®
    expired_sessions = r.keys('session:*')
    expired_cache = r.keys('cache:temp:*')
    
    for key in expired_sessions:
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        ttl = r.ttl(key)
        if ttl == -1:  # æ²¡æœ‰è®¾ç½®è¿‡æœŸæ—¶é—´çš„æ—§æ•°æ®
            pipe.delete(key)
    
    for key in expired_cache:
        # æ¸…ç†ä¸´æ—¶ç¼“å­˜
        pipe.delete(key)
    
    # æ¸…ç†è¿‡æœŸçš„æ’è¡Œæ¦œæ•°æ®
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    pipe.delete(f"leaderboard:daily:{yesterday}")
    
    results = pipe.execute()
    return sum(1 for result in results if result)
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### ç®¡é“å¤§å°ä¼˜åŒ–
```python
def optimized_batch_operation(data, batch_size=100):
    """ä¼˜åŒ–çš„æ‰¹é‡æ“ä½œï¼Œæ§åˆ¶ç®¡é“å¤§å°"""
    total_processed = 0
    
    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]
        pipe = r.pipeline()
        
        for item in batch:
            pipe.set(f"key:{item['id']}", item['value'])
            pipe.expire(f"key:{item['id']}", 3600)
        
        pipe.execute()
        total_processed += len(batch)
        
        # å¯é€‰ï¼šæ·»åŠ è¿›åº¦æŠ¥å‘Š
        if total_processed % 1000 == 0:
            print(f"å·²å¤„ç† {total_processed} æ¡è®°å½•")
    
    return total_processed
```

### é”™è¯¯å¤„ç†
```python
def robust_pipeline_operation(operations):
    """å¸¦é”™è¯¯å¤„ç†çš„ç®¡é“æ“ä½œ"""
    pipe = r.pipeline()
    
    try:
        for operation in operations:
            if operation['type'] == 'set':
                pipe.set(operation['key'], operation['value'])
            elif operation['type'] == 'incr':
                pipe.incr(operation['key'])
            elif operation['type'] == 'sadd':
                pipe.sadd(operation['key'], *operation['members'])
        
        results = pipe.execute()
        return {'success': True, 'results': results}
        
    except redis.RedisError as e:
        return {'success': False, 'error': str(e)}
    except Exception as e:
        return {'success': False, 'error': f"Unexpected error: {str(e)}"}
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### å†…å­˜ä½¿ç”¨
```python
# ç®¡é“ä¼šåœ¨å®¢æˆ·ç«¯ç¼“å­˜å‘½ä»¤ï¼Œå¤§é‡å‘½ä»¤ä¼šå ç”¨å†…å­˜
# å»ºè®®æ§åˆ¶ç®¡é“å¤§å°
MAX_PIPELINE_SIZE = 1000

def safe_pipeline_operation(commands):
    results = []
    
    for i in range(0, len(commands), MAX_PIPELINE_SIZE):
        batch = commands[i:i + MAX_PIPELINE_SIZE]
        pipe = r.pipeline()
        
        for cmd in batch:
            getattr(pipe, cmd['method'])(*cmd['args'])
        
        batch_results = pipe.execute()
        results.extend(batch_results)
    
    return results
```

### äº‹åŠ¡æ€§è€ƒè™‘
```python
# ç®¡é“ä¸ä¿è¯äº‹åŠ¡æ€§ï¼Œå¦‚æœéœ€è¦äº‹åŠ¡ï¼Œä½¿ç”¨ MULTI/EXEC
def transactional_pipeline():
    pipe = r.pipeline(transaction=True)  # å¯ç”¨äº‹åŠ¡æ¨¡å¼
    
    pipe.multi()  # å¼€å§‹äº‹åŠ¡
    pipe.set('key1', 'value1')
    pipe.incr('counter')
    pipe.set('key2', 'value2')
    
    try:
        results = pipe.execute()  # åŸå­æ‰§è¡Œ
        return results
    except redis.WatchError:
        return None  # äº‹åŠ¡è¢«ä¸­æ–­
```

## ğŸ›¡ï¸ æœ€ä½³å®è·µ

### 1. åˆç†æ§åˆ¶ç®¡é“å¤§å°
```python
# æ¨èçš„ç®¡é“å¤§å°èŒƒå›´ï¼š100-1000 ä¸ªå‘½ä»¤
OPTIMAL_PIPELINE_SIZE = 500

def batch_process(items):
    for i in range(0, len(items), OPTIMAL_PIPELINE_SIZE):
        batch = items[i:i + OPTIMAL_PIPELINE_SIZE]
        process_batch(batch)
```

### 2. ç›‘æ§å’Œæ—¥å¿—
```python
import logging

def monitored_pipeline_operation(operations):
    start_time = time.time()
    pipe = r.pipeline()
    
    for op in operations:
        pipe.set(op['key'], op['value'])
    
    results = pipe.execute()
    
    execution_time = time.time() - start_time
    logging.info(f"ç®¡é“æ‰§è¡Œå®Œæˆ: {len(operations)} ä¸ªå‘½ä»¤, "
                f"è€—æ—¶: {execution_time:.3f} ç§’")
    
    return results
```

### 3. é”™è¯¯æ¢å¤
```python
def resilient_pipeline_operation(operations, max_retries=3):
    for attempt in range(max_retries):
        try:
            pipe = r.pipeline()
            for op in operations:
                getattr(pipe, op['method'])(*op['args'])
            
            return pipe.execute()
            
        except redis.ConnectionError:
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

### 4. æ€§èƒ½ç›‘æ§
```python
def performance_aware_pipeline(operations):
    if len(operations) > 10000:
        logging.warning(f"å¤§å‹ç®¡é“æ“ä½œ: {len(operations)} ä¸ªå‘½ä»¤")
    
    start_memory = psutil.Process().memory_info().rss
    start_time = time.time()
    
    # æ‰§è¡Œç®¡é“æ“ä½œ
    results = execute_pipeline(operations)
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss
    
    logging.info(f"ç®¡é“æ€§èƒ½ç»Ÿè®¡: "
                f"å‘½ä»¤æ•°: {len(operations)}, "
                f"è€—æ—¶: {end_time - start_time:.3f}s, "
                f"å†…å­˜å¢é•¿: {(end_memory - start_memory) / 1024 / 1024:.2f}MB")
    
    return results
```

---

*Redis ç®¡é“æ˜¯æé«˜æ‰¹é‡æ“ä½œæ€§èƒ½çš„é‡è¦å·¥å…·ï¼Œåˆç†ä½¿ç”¨å¯ä»¥æ˜¾è‘—æå‡åº”ç”¨æ€§èƒ½ï¼Œä½†éœ€è¦æ³¨æ„å†…å­˜ä½¿ç”¨å’Œé”™è¯¯å¤„ç†ï¼*
