# Docker æ—¥å¿—ç®¡ç†

å®¹å™¨æ—¥å¿—ç®¡ç†æ˜¯è¿ç»´ç›‘æ§çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æœ¬æ–‡æ¡£ä»‹ç» Docker æ—¥å¿—çš„æ”¶é›†ã€å­˜å‚¨ã€åˆ†æå’Œæœ€ä½³å®è·µã€‚

## ğŸ“ æ—¥å¿—åŸºç¡€æ¦‚å¿µ

### Docker æ—¥å¿—é©±åŠ¨

Docker æ”¯æŒå¤šç§æ—¥å¿—é©±åŠ¨ï¼š

| é©±åŠ¨ | æè¿° | ä½¿ç”¨åœºæ™¯ |
|------|------|----------|
| json-file | é»˜è®¤é©±åŠ¨ï¼ŒJSON æ ¼å¼ | å¼€å‘å’Œå°å‹éƒ¨ç½² |
| syslog | ç³»ç»Ÿæ—¥å¿— | é›†æˆç°æœ‰æ—¥å¿—ç³»ç»Ÿ |
| journald | systemd æ—¥å¿— | systemd ç³»ç»Ÿ |
| fluentd | Fluentd æ—¥å¿—æ”¶é›† | å¤§è§„æ¨¡æ—¥å¿—æ”¶é›† |
| awslogs | AWS CloudWatch | AWS ç¯å¢ƒ |
| gcplogs | Google Cloud Logging | GCP ç¯å¢ƒ |
| none | ç¦ç”¨æ—¥å¿— | ä¸éœ€è¦æ—¥å¿—çš„å®¹å™¨ |

### æŸ¥çœ‹å½“å‰æ—¥å¿—é…ç½®

```bash
# æŸ¥çœ‹ Docker å®ˆæŠ¤è¿›ç¨‹æ—¥å¿—é…ç½®
docker info | grep -A 10 "Logging Driver"

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—é…ç½®
docker inspect container-name | grep -A 10 "LogConfig"
```

## ğŸ“Š åŸºç¡€æ—¥å¿—æ“ä½œ

### æŸ¥çœ‹å®¹å™¨æ—¥å¿—

```bash
# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs container-name

# å®æ—¶æŸ¥çœ‹æ—¥å¿—
docker logs -f container-name

# æŸ¥çœ‹æœ€è¿‘çš„æ—¥å¿—
docker logs --tail 100 container-name

# æŸ¥çœ‹æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ—¥å¿—
docker logs --since "2024-01-01T00:00:00" container-name
docker logs --until "2024-01-01T23:59:59" container-name

# æ˜¾ç¤ºæ—¶é—´æˆ³
docker logs -t container-name

# ç»„åˆä½¿ç”¨
docker logs -f --tail 50 --since "1h" container-name
```

### æ—¥å¿—æ ¼å¼åŒ–

```bash
# æŸ¥çœ‹ JSON æ ¼å¼æ—¥å¿—
docker logs --details container-name

# ä½¿ç”¨ jq æ ¼å¼åŒ–æ—¥å¿—
docker logs container-name 2>&1 | jq '.'

# æå–ç‰¹å®šå­—æ®µ
docker logs container-name 2>&1 | jq '.log'
```

## ğŸ”§ æ—¥å¿—é©±åŠ¨é…ç½®

### å…¨å±€æ—¥å¿—é…ç½®

```json
# /etc/docker/daemon.json
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3",
    "labels": "production_status",
    "env": "os,customer"
  }
}
```

```bash
# é‡å¯ Docker æœåŠ¡ä½¿é…ç½®ç”Ÿæ•ˆ
sudo systemctl restart docker
```

### å®¹å™¨çº§åˆ«æ—¥å¿—é…ç½®

```bash
# è¿è¡Œæ—¶æŒ‡å®šæ—¥å¿—é©±åŠ¨
docker run -d \
  --log-driver json-file \
  --log-opt max-size=10m \
  --log-opt max-file=3 \
  --name web nginx

# ä½¿ç”¨ syslog é©±åŠ¨
docker run -d \
  --log-driver syslog \
  --log-opt syslog-address=tcp://192.168.1.100:514 \
  --log-opt tag="{{.Name}}/{{.FullID}}" \
  --name app my-app
```

### Docker Compose æ—¥å¿—é…ç½®

```yaml
services:
  web:
    image: nginx
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=web,environment=production"
    
  api:
    image: my-api
    logging:
      driver: "syslog"
      options:
        syslog-address: "tcp://log-server:514"
        tag: "api-{{.Name}}"
    
  db:
    image: postgres:15
    logging:
      driver: "fluentd"
      options:
        fluentd-address: "fluentd:24224"
        tag: "database.{{.Name}}"
```

## ğŸ—ï¸ é›†ä¸­åŒ–æ—¥å¿—æ”¶é›†

### ELK Stack éƒ¨ç½²

```yaml
# docker-compose.elk.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: logstash
    ports:
      - "5044:5044"
      - "9600:9600"
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
      - ./logstash/config:/usr/share/logstash/config
    depends_on:
      - elasticsearch
    
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - elasticsearch
      - logstash

volumes:
  elasticsearch-data:
```

### Filebeat é…ç½®

```yaml
# filebeat/filebeat.yml
filebeat.inputs:
- type: container
  paths:
    - '/var/lib/docker/containers/*/*.log'
  processors:
    - add_docker_metadata:
        host: "unix:///var/run/docker.sock"

output.logstash:
  hosts: ["logstash:5044"]

logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

### Logstash é…ç½®

```ruby
# logstash/pipeline/logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [container][name] {
    mutate {
      add_field => { "service_name" => "%{[container][name]}" }
    }
  }
  
  # è§£æ JSON æ—¥å¿—
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }
  
  # æ·»åŠ æ—¶é—´æˆ³
  date {
    match => [ "timestamp", "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "docker-logs-%{+YYYY.MM.dd}"
  }
  
  stdout {
    codec => rubydebug
  }
}
```

## ğŸ“ˆ Fluentd æ—¥å¿—æ”¶é›†

### Fluentd éƒ¨ç½²

```yaml
# docker-compose.fluentd.yml
services:
  fluentd:
    image: fluent/fluentd:v1.16-debian-1
    container_name: fluentd
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - ./fluentd/conf:/fluentd/etc
      - ./fluentd/logs:/var/log/fluentd
    
  app:
    image: my-app
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: "app.{{.Name}}"
```

### Fluentd é…ç½®

```xml
<!-- fluentd/conf/fluent.conf -->
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

<filter app.**>
  @type parser
  key_name log
  <parse>
    @type json
  </parse>
</filter>

<match app.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name docker-logs
  type_name _doc
  <buffer>
    @type file
    path /var/log/fluentd/buffer
    flush_mode interval
    flush_interval 10s
  </buffer>
</match>

<match **>
  @type stdout
</match>
```

## ğŸ” æ—¥å¿—åˆ†æå’ŒæŸ¥è¯¢

### Elasticsearch æŸ¥è¯¢

```bash
# åŸºç¡€æŸ¥è¯¢
curl -X GET "localhost:9200/docker-logs-*/_search?pretty" \
  -H 'Content-Type: application/json' \
  -d '{
    "query": {
      "match": {
        "service_name": "web"
      }
    }
  }'

# æ—¶é—´èŒƒå›´æŸ¥è¯¢
curl -X GET "localhost:9200/docker-logs-*/_search?pretty" \
  -H 'Content-Type: application/json' \
  -d '{
    "query": {
      "bool": {
        "must": [
          {"match": {"service_name": "api"}},
          {"range": {"@timestamp": {"gte": "now-1h"}}}
        ]
      }
    }
  }'
```

### Kibana ä»ªè¡¨æ¿

```json
{
  "version": "8.11.0",
  "objects": [
    {
      "id": "docker-logs-dashboard",
      "type": "dashboard",
      "attributes": {
        "title": "Docker Logs Dashboard",
        "hits": 0,
        "description": "Docker container logs monitoring",
        "panelsJSON": "[{\"version\":\"8.11.0\",\"gridData\":{\"x\":0,\"y\":0,\"w\":24,\"h\":15,\"i\":\"1\"},\"panelIndex\":\"1\",\"embeddableConfig\":{},\"panelRefName\":\"panel_1\"}]"
      }
    }
  ]
}
```

## ğŸš¨ æ—¥å¿—å‘Šè­¦

### Elastalert é…ç½®

```yaml
# elastalert/rules/error_alert.yml
name: Docker Error Alert
type: frequency
index: docker-logs-*
num_events: 5
timeframe:
  minutes: 5

filter:
- terms:
    level: ["error", "ERROR", "Error"]

alert:
- "email"
- "slack"

email:
- "admin@example.com"

slack:
webhook_url: "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
slack_channel_override: "#alerts"
slack_username_override: "ElastAlert"
```

### Prometheus + Grafana ç›‘æ§

```yaml
# docker-compose.monitoring.yml
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    
  node-exporter:
    image: prom/node-exporter
    ports:
      - "9100:9100"
    
  cadvisor:
    image: gcr.io/cadvisor/cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro

volumes:
  grafana-data:
```

## ğŸ› ï¸ æ—¥å¿—ç®¡ç†å·¥å…·

### æ—¥å¿—è½®è½¬è„šæœ¬

```bash
#!/bin/bash
# log-rotation.sh

# å‹ç¼©æ—§æ—¥å¿—
find /var/lib/docker/containers -name "*.log" -size +100M -exec gzip {} \;

# åˆ é™¤è¶…è¿‡ 30 å¤©çš„æ—¥å¿—
find /var/lib/docker/containers -name "*.log.gz" -mtime +30 -delete

# æ¸…ç† Docker æ—¥å¿—
docker system prune -f --filter "until=720h"
```

### æ—¥å¿—å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup-logs.sh

DATE=$(date +%Y%m%d)
BACKUP_DIR="/backup/docker-logs"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½å®¹å™¨æ—¥å¿—
for container in $(docker ps -q); do
  container_name=$(docker inspect --format='{{.Name}}' $container | sed 's/\///')
  docker logs $container > $BACKUP_DIR/${container_name}_${DATE}.log
done

# å‹ç¼©å¤‡ä»½
tar -czf $BACKUP_DIR/docker-logs-${DATE}.tar.gz $BACKUP_DIR/*.log
rm $BACKUP_DIR/*.log
```

## ğŸš€ æœ€ä½³å®è·µ

### 1. ç»“æ„åŒ–æ—¥å¿—

```javascript
// åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—
const winston = require('winston');

const logger = winston.createLogger({
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [
    new winston.transports.Console()
  ]
});

logger.info('User login', {
  userId: '12345',
  ip: '192.168.1.100',
  userAgent: 'Mozilla/5.0...'
});
```

### 2. æ—¥å¿—çº§åˆ«ç®¡ç†

```yaml
services:
  app:
    image: my-app
    environment:
      - LOG_LEVEL=info
      - DEBUG=app:*
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
```

### 3. æ•æ„Ÿä¿¡æ¯è¿‡æ»¤

```ruby
# logstash è¿‡æ»¤æ•æ„Ÿä¿¡æ¯
filter {
  mutate {
    gsub => [
      "message", "password=[^&\s]*", "password=***",
      "message", "token=[^&\s]*", "token=***"
    ]
  }
}
```

### 4. æ€§èƒ½ä¼˜åŒ–

```yaml
services:
  app:
    image: my-app
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"  # å‹ç¼©æ—¥å¿—æ–‡ä»¶
```

### 5. ç›‘æ§æ—¥å¿—å­˜å‚¨

```bash
# ç›‘æ§æ—¥å¿—ç£ç›˜ä½¿ç”¨
df -h /var/lib/docker

# æ¸…ç†æ—¥å¿—
docker system prune --volumes -f
```

é€šè¿‡åˆç†çš„æ—¥å¿—ç®¡ç†ç­–ç•¥ï¼Œæ‚¨å¯ä»¥æœ‰æ•ˆç›‘æ§å®¹å™¨åŒ–åº”ç”¨çš„è¿è¡ŒçŠ¶æ€ï¼Œå¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜ã€‚
